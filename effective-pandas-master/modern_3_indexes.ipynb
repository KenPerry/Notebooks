{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From https://github.com/TomAugspurger/effective-pandas/tree/updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexes\n",
    "\n",
    "Today we're going to be talking about pandas' [`Index`es](http://pandas.pydata.org/pandas-docs/version/0.18.0/api.html#index).\n",
    "They're essential to pandas, but can be a difficult concept to grasp at first.\n",
    "I suspect this is partly because they're unlike what you'll find in SQL or R.\n",
    "\n",
    "`Index`es offer\n",
    "\n",
    "- a metadata container\n",
    "- easy label-based row selection and assignment\n",
    "- easy label-based alignment in operations\n",
    "\n",
    "One of my first tasks when analyzing a new dataset is to identify a unique identifier for each observation, and set that as the index. It could be a simple integer, or like in our first chapter, it could be several columns (`carrier`, `origin` `dest`, `tail_num` `date`).\n",
    "\n",
    "To demonstrate the benefits of proper `Index` use, we'll first fetch some weather data from sensors at a bunch of airports across the US.\n",
    "See [here](https://github.com/akrherz/iem/blob/master/scripts/asos/iem_scraper_example.py) for the example scraper I based this off of.\n",
    "Those uninterested in the details of fetching and prepping the data and [skip past it](#set-operations).\n",
    "\n",
    "At a high level, here's how we'll fetch the data: the sensors are broken up by \"network\" (states).\n",
    "We'll make one API call per state to get the list of airport IDs per network (using `get_ids` below).\n",
    "Once we have the IDs, we'll again make one call per state getting the actual observations (in `get_weather`).\n",
    "Feel free to skim the code below, I'll highlight the interesting bits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from io import StringIO\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import prep\n",
    "\n",
    "sns.set_style('ticks')\n",
    "pd.options.display.max_rows = 10\n",
    "# States are broken into networks. The networks have a list of ids, each representing a station.\n",
    "# We will take that list of ids and pass them as query parameters to the URL we built up ealier.\n",
    "states = \"\"\"AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME\n",
    " MI MN MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT\n",
    " WA WI WV WY\"\"\".split()\n",
    "\n",
    "# IEM has Iowa AWOS sites in its own labeled network\n",
    "networks = ['AWOS'] + ['{}_ASOS'.format(state) for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weather(stations, start=pd.Timestamp('2017-01-01'),\n",
    "                end=pd.Timestamp('2017-01-31')):\n",
    "    '''\n",
    "    Fetch weather data from MESONet between ``start`` and ``stop``.\n",
    "    '''\n",
    "    url = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "           \"&data=tmpf&data=relh&data=sped&data=mslp&data=p01i&data=v\"\n",
    "           \"sby&data=gust_mph&data=skyc1&data=skyc2&data=skyc3\"\n",
    "           \"&tz=Etc/UTC&format=comma&latlon=no\"\n",
    "           \"&{start:year1=%Y&month1=%m&day1=%d}\"\n",
    "           \"&{end:year2=%Y&month2=%m&day2=%d}&{stations}\")\n",
    "    stations = \"&\".join(\"station=%s\" % s for s in stations)\n",
    "    weather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n",
    "                           comment=\"#\")\n",
    "                 .rename(columns={\"valid\": \"date\"})\n",
    "                 .rename(columns=str.strip)\n",
    "                 .assign(date=lambda df: pd.to_datetime(df['date']))\n",
    "                 .set_index([\"station\", \"date\"])\n",
    "                 .sort_index())\n",
    "    float_cols = ['tmpf', 'relh', 'sped', 'mslp', 'p01i', 'vsby', \"gust_mph\"]\n",
    "    weather[float_cols] = weather[float_cols].apply(pd.to_numeric, errors=\"corce\")\n",
    "    return weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ids(network):\n",
    "    url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\n",
    "    r = requests.get(url.format(network))\n",
    "    md = pd.io.json.json_normalize(r.json()['features'])\n",
    "    md['network'] = network\n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't too much in `get_weather` worth mentioning, just grabbing some CSV files from various URLs.\n",
    "They put metadata in the \"CSV\"s at the top of the file as lines prefixed by a `#`.\n",
    "Pandas will ignore these with the `comment='#'` parameter.\n",
    "\n",
    "I do want to talk briefly about the gem of a method that is [`json_normalize`](http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.io.json.json_normalize.html)  .\n",
    "The weather API returns some slightly-nested data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\n",
    "r = requests.get(url.format(\"AWOS\"))\n",
    "js = r.json()\n",
    "\n",
    "js['features'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just pass that list off to the `DataFrame` constructor, we get this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(js['features']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, DataFrames don't handle nested data that well.\n",
    "It's often better to normalize it somehow.\n",
    "In this case, we can \"lift\"\n",
    "the nested items (`geometry.coordinates`, `properties.sid`, and `properties.sname`)\n",
    "up to the top level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.io.json.json_normalize(js['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, it's not *that* difficult to write a quick for loop or list comprehension to extract those,\n",
    "but that gets tedious.\n",
    "If we were using the latitude and longitude data, we would want to split\n",
    "the `geometry.coordinates` column into two. But we aren't so we won't.\n",
    "\n",
    "Going back to the task, we get the airport IDs for every network (state)\n",
    "with `get_ids`. Then we pass those IDs into `get_weather` to fetch the\n",
    "actual weather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running out of memory reading all .csv files, limit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ids = pd.concat([get_ids(network) for network in networks], ignore_index=True)\n",
    "gr = ids.groupby('network')\n",
    "\n",
    "store = 'data/weather.h5'\n",
    "\n",
    "if not os.path.exists(store):\n",
    "    os.makedirs(\"data/weather\", exist_ok=True)\n",
    "\n",
    "    for k, v in gr:\n",
    "        weather = get_weather(v['id'])\n",
    "        weather.to_csv(\"data/weather/{}.csv\".format(k))\n",
    "\n",
    "    weather = pd.concat([\n",
    "        pd.read_csv(f, parse_dates=['date'], index_col=['station', 'date'])\n",
    "        for f in glob.glob('data/weather/*.csv')\n",
    "    ]).sort_index()\n",
    "\n",
    "    weather.to_hdf(\"data/weather.h5\", \"weather\")\n",
    "else:\n",
    "    weather = pd.read_hdf(\"data/weather.h5\", \"weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tmpf</th>\n",
       "      <th>relh</th>\n",
       "      <th>sped</th>\n",
       "      <th>mslp</th>\n",
       "      <th>p01i</th>\n",
       "      <th>vsby</th>\n",
       "      <th>gust_mph</th>\n",
       "      <th>skyc1</th>\n",
       "      <th>skyc2</th>\n",
       "      <th>skyc3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">05U</th>\n",
       "      <th>2017-01-01 00:15:00</th>\n",
       "      <td>32.0</td>\n",
       "      <td>59.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:35:00</th>\n",
       "      <td>26.6</td>\n",
       "      <td>68.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:55:00</th>\n",
       "      <td>23.0</td>\n",
       "      <td>73.51</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 01:15:00</th>\n",
       "      <td>21.2</td>\n",
       "      <td>79.31</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 01:35:00</th>\n",
       "      <td>19.4</td>\n",
       "      <td>79.16</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             tmpf   relh  sped  mslp  p01i  vsby  gust_mph  \\\n",
       "station date                                                                 \n",
       "05U     2017-01-01 00:15:00  32.0  59.27   0.0   NaN   0.0  10.0       NaN   \n",
       "        2017-01-01 00:35:00  26.6  68.40   0.0   NaN   0.0  10.0       NaN   \n",
       "        2017-01-01 00:55:00  23.0  73.51   5.8   NaN   0.0  10.0       NaN   \n",
       "        2017-01-01 01:15:00  21.2  79.31   6.9   NaN   0.0  10.0       NaN   \n",
       "        2017-01-01 01:35:00  19.4  79.16   5.8   NaN   0.0  10.0       NaN   \n",
       "\n",
       "                            skyc1 skyc2 skyc3  \n",
       "station date                                   \n",
       "05U     2017-01-01 00:15:00   CLR     M     M  \n",
       "        2017-01-01 00:35:00   CLR     M     M  \n",
       "        2017-01-01 00:55:00   CLR     M     M  \n",
       "        2017-01-01 01:15:00   CLR     M     M  \n",
       "        2017-01-01 01:35:00   CLR     M     M  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_hdf(\"data/weather.h5\", \"weather\")\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that was a bit of work. Here's a plot to reward ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index is a MultiIndex (station, date)\n",
    "### So can easily select rows by station, e.g., in list airports.\n",
    "### Need to do a \"reset_index\" to move station from Index to regular column so can be used in FacetGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(weather.reset_index().station.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have to modify airports, since we truncated weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather.index.levels[0].unique().tolist()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#airports = ['W43', 'AFO', '82V', 'DUB']\n",
    "airports = weather.index.levels[0].unique().tolist()[:4]\n",
    "g = sns.FacetGrid(weather.loc[airports].reset_index(),\n",
    "                  col='station', hue='station', col_wrap=2, size=4)\n",
    "g.map(sns.regplot, 'sped', 'gust_mph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Operations\n",
    "\n",
    "Indexes are set-like (technically *multi*sets, since you can have duplicates), so they support most python `set` operations. Since indexes are immutable you won't find any of the inplace `set` operations.\n",
    "One other difference is that since `Index`es are also array-like, you can't use some infix operators like `-` for `difference`. If you have a numeric index it is unclear whether you intend to perform math operations or set operations.\n",
    "You can use `&` for intersection, `|` for union, and `^` for symmetric difference though, since there's no ambiguity.\n",
    "\n",
    "For example, lets find the set of airports that we have both weather and flight information on. Since `weather` had a MultiIndex of `airport, datetime`, we'll use the `levels` attribute to get at the airport data, separate from the date data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450017, 33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = pd.read_hdf('data/flights.h5', 'flights')\n",
    "flights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flights.loc[:,'origin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bring in the flights data\n",
    "\n",
    "#flights = pd.read_hdf('data/flights.h5', 'flights')\n",
    "\n",
    "weather_locs = weather.index.levels[0]\n",
    "# The `categories` attribute of a Categorical is an Index\n",
    "origin_locs = flights.origin.cat.categories\n",
    "dest_locs = flights.dest.cat.categories\n",
    "\n",
    "airports = weather_locs & origin_locs & dest_locs\n",
    "airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Weather, no flights:\\n\\t\", weather_locs.difference(origin_locs | dest_locs), end='\\n\\n')\n",
    "\n",
    "print(\"Flights, no weather:\\n\\t\", (origin_locs | dest_locs).difference(weather_locs), end='\\n\\n')\n",
    "\n",
    "print(\"Dropped Stations:\\n\\t\", (origin_locs | dest_locs) ^ weather_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flavors\n",
    "\n",
    "Pandas has many subclasses of the regular `Index`, each tailored to a specific kind of data.\n",
    "Most of the time these will be created for you automatically, so you don't have to worry about which one to choose.\n",
    "\n",
    "1. [`Index`](http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.Index.html#pandas.Index)\n",
    "2. `Int64Index`\n",
    "3. `RangeIndex`: Memory-saving special case of `Int64Index`\n",
    "4. `FloatIndex`\n",
    "5. `DatetimeIndex`: Datetime64[ns] precision data\n",
    "6. `PeriodIndex`: Regularly-spaced, arbitrary precision datetime data.\n",
    "7. `TimedeltaIndex`\n",
    "8. `CategoricalIndex`\n",
    "9. `MultiIndex`\n",
    "\n",
    "You will sometimes create a `DatetimeIndex` with [`pd.date_range`](http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.date_range.html) ([`pd.period_range`](http://pandas.pydata.org/pandas-docs/version/0.18.0/generated/pandas.period_range.html) for `PeriodIndex`).\n",
    "And you'll sometimes make a `MultiIndex` directly too (I'll have an example of this in my post on performace).\n",
    "\n",
    "Some of these specialized index types are purely optimizations; others use information about the data to provide additional methods.\n",
    "And while you might occasionally work with indexes directly (like the set operations above), most of they time you'll be operating on a Series or DataFrame, which in turn makes use of its Index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Slicing\n",
    "We saw in part one that they're great for making *row* subsetting as easy as column subsetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather.loc['DSM'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without indexes we'd probably resort to boolean masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather2 = weather.reset_index()\n",
    "weather2[weather2['station'] == 'DSM'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly less convenient, but still doable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes for Easier Arithmetic, Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's nice to have your metadata (labels on each observation) next to you actual values. But if you store them in an array, they'll get in the way of your operations.\n",
    "Say we wanted to translate the Fahrenheit temperature to Celsius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: when selected column from Dataframe with index, results is a Series with same index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'station'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'station'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.index.levels[0].name\n",
    "weather['tmpf'].index.levels[0].name\n",
    "\n",
    "weather.index is weather['tmpf'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With indecies\n",
    "temp = weather['tmpf']\n",
    "\n",
    "c = (temp - 32) * 5 / 9\n",
    "c.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# without\n",
    "temp2 = weather.reset_index()[['station', 'date', 'tmpf']]\n",
    "\n",
    "temp2['tmpf'] = (temp2['tmpf'] - 32) * 5 / 9\n",
    "temp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not terrible, but not as good.\n",
    "And, what if you had wanted to keep Fahrenheit around as well, instead of overwriting it like we did?\n",
    "Then you'd need to make a copy of everything, including the `station` and `date` columns.\n",
    "We don't have that problem, since indexes are immutable and safely shared between DataFrames / Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp.index is c.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes for Alignment\n",
    "\n",
    "I've saved the best for last.\n",
    "Automatic alignment, or reindexing, is fundamental to pandas.\n",
    "\n",
    "All binary operations (add, multiply, etc.) between Series/DataFrames first *align* and then proceed.\n",
    "\n",
    "Let's suppose we have hourly observations on temperature and windspeed.\n",
    "And suppose some of the observations were invalid, and not reported (simulated below by sampling from the full dataset). We'll assume the missing windspeed observations were potentially different from the missing temperature observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A MultiIndex member is a tuple\n",
    "### Selecting via one element of the tuple results in a dataframe with a MultiIndex whoses tuples are one fewer in count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenList(['station', 'date'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "FrozenList(['date'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.index.names\n",
    "weather.loc[ weather.index[0][0] ].index.names\n",
    "\n",
    "weather.index.names[-1]  is weather.loc[ weather.index[0][0] ].index.names[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmpf</th>\n",
       "      <th>relh</th>\n",
       "      <th>sped</th>\n",
       "      <th>mslp</th>\n",
       "      <th>p01i</th>\n",
       "      <th>vsby</th>\n",
       "      <th>gust_mph</th>\n",
       "      <th>skyc1</th>\n",
       "      <th>skyc2</th>\n",
       "      <th>skyc3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:15:00</th>\n",
       "      <td>32.0</td>\n",
       "      <td>59.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:35:00</th>\n",
       "      <td>26.6</td>\n",
       "      <td>68.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:55:00</th>\n",
       "      <td>23.0</td>\n",
       "      <td>73.51</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 01:15:00</th>\n",
       "      <td>21.2</td>\n",
       "      <td>79.31</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 01:35:00</th>\n",
       "      <td>19.4</td>\n",
       "      <td>79.16</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 22:35:00</th>\n",
       "      <td>39.2</td>\n",
       "      <td>48.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 22:55:00</th>\n",
       "      <td>39.2</td>\n",
       "      <td>51.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 23:15:00</th>\n",
       "      <td>39.2</td>\n",
       "      <td>51.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 23:35:00</th>\n",
       "      <td>39.2</td>\n",
       "      <td>51.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 23:55:00</th>\n",
       "      <td>35.6</td>\n",
       "      <td>59.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2124 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tmpf   relh  sped  mslp  p01i  vsby  gust_mph skyc1  \\\n",
       "date                                                                       \n",
       "2017-01-01 00:15:00  32.0  59.27   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-01 00:35:00  26.6  68.40   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-01 00:55:00  23.0  73.51   5.8   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-01 01:15:00  21.2  79.31   6.9   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-01 01:35:00  19.4  79.16   5.8   NaN   0.0  10.0       NaN   CLR   \n",
       "...                   ...    ...   ...   ...   ...   ...       ...   ...   \n",
       "2017-01-30 22:35:00  39.2  48.10   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-30 22:55:00  39.2  51.89   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-30 23:15:00  39.2  51.89   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-30 23:35:00  39.2  51.89   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-30 23:55:00  35.6  59.79   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "\n",
       "                    skyc2 skyc3  \n",
       "date                             \n",
       "2017-01-01 00:15:00     M     M  \n",
       "2017-01-01 00:35:00     M     M  \n",
       "2017-01-01 00:55:00     M     M  \n",
       "2017-01-01 01:15:00     M     M  \n",
       "2017-01-01 01:35:00     M     M  \n",
       "...                   ...   ...  \n",
       "2017-01-30 22:35:00     M     M  \n",
       "2017-01-30 22:55:00     M     M  \n",
       "2017-01-30 23:15:00     M     M  \n",
       "2017-01-30 23:35:00     M     M  \n",
       "2017-01-30 23:55:00     M     M  \n",
       "\n",
       "[2124 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.loc[ weather.index[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('05U', Timestamp('2017-01-01 00:15:00'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'05U'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.loc[first_index].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmpf</th>\n",
       "      <th>relh</th>\n",
       "      <th>sped</th>\n",
       "      <th>mslp</th>\n",
       "      <th>p01i</th>\n",
       "      <th>vsby</th>\n",
       "      <th>gust_mph</th>\n",
       "      <th>skyc1</th>\n",
       "      <th>skyc2</th>\n",
       "      <th>skyc3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:15:00</th>\n",
       "      <td>32.0</td>\n",
       "      <td>59.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:35:00</th>\n",
       "      <td>26.6</td>\n",
       "      <td>68.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:55:00</th>\n",
       "      <td>23.0</td>\n",
       "      <td>73.51</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 01:15:00</th>\n",
       "      <td>21.2</td>\n",
       "      <td>79.31</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 01:35:00</th>\n",
       "      <td>19.4</td>\n",
       "      <td>79.16</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 22:35:00</th>\n",
       "      <td>39.2</td>\n",
       "      <td>48.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 22:55:00</th>\n",
       "      <td>39.2</td>\n",
       "      <td>51.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 23:15:00</th>\n",
       "      <td>39.2</td>\n",
       "      <td>51.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 23:35:00</th>\n",
       "      <td>39.2</td>\n",
       "      <td>51.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-30 23:55:00</th>\n",
       "      <td>35.6</td>\n",
       "      <td>59.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CLR</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2124 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tmpf   relh  sped  mslp  p01i  vsby  gust_mph skyc1  \\\n",
       "date                                                                       \n",
       "2017-01-01 00:15:00  32.0  59.27   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-01 00:35:00  26.6  68.40   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-01 00:55:00  23.0  73.51   5.8   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-01 01:15:00  21.2  79.31   6.9   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-01 01:35:00  19.4  79.16   5.8   NaN   0.0  10.0       NaN   CLR   \n",
       "...                   ...    ...   ...   ...   ...   ...       ...   ...   \n",
       "2017-01-30 22:35:00  39.2  48.10   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-30 22:55:00  39.2  51.89   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-30 23:15:00  39.2  51.89   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-30 23:35:00  39.2  51.89   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "2017-01-30 23:55:00  35.6  59.79   0.0   NaN   0.0  10.0       NaN   CLR   \n",
       "\n",
       "                    skyc2 skyc3  \n",
       "date                             \n",
       "2017-01-01 00:15:00     M     M  \n",
       "2017-01-01 00:35:00     M     M  \n",
       "2017-01-01 00:55:00     M     M  \n",
       "2017-01-01 01:15:00     M     M  \n",
       "2017-01-01 01:35:00     M     M  \n",
       "...                   ...   ...  \n",
       "2017-01-30 22:35:00     M     M  \n",
       "2017-01-30 22:55:00     M     M  \n",
       "2017-01-30 23:15:00     M     M  \n",
       "2017-01-30 23:35:00     M     M  \n",
       "2017-01-30 23:55:00     M     M  \n",
       "\n",
       "[2124 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsm = weather.loc[ weather.index.levels[0][0]]\n",
    "dsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dsm = weather.loc['DSM']\n",
    "dsm = weather.loc[ weather.index.levels[0][0]]\n",
    "\n",
    "hourly = dsm.resample('H').mean()\n",
    "\n",
    "temp = hourly['tmpf'].sample(frac=.5, random_state=1).sort_index()\n",
    "sped = hourly['sped'].sample(frac=.5, random_state=2).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp.head().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the two indexes aren't identical.\n",
    "\n",
    "Suppose that the `windspeed : temperature` ratio is meaningful.\n",
    "When we go to compute that, pandas will automatically align the two by index label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sped / temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets you focus on doing the operation, rather than manually aligning things, ensuring that the arrays are the same length and in the same order.\n",
    "By deault, missing values are inserted where the two don't align.\n",
    "You can use the method version of any binary operation to specify a `fill_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sped.div(temp, fill_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since I couldn't find anywhere else to put it, you can control the axis the operation is aligned along as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourly.div(sped, axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non row-labeled version of this is messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp2 = temp.reset_index()\n",
    "sped2 = sped.reset_index()\n",
    "\n",
    "# Find rows where the operation is defined\n",
    "common_dates = pd.Index(temp2.date) & sped2.date\n",
    "pd.concat([\n",
    "    # concat to not lose date information\n",
    "    sped2.loc[sped2['date'].isin(common_dates), 'date'],\n",
    "    (sped2.loc[sped2.date.isin(common_dates), 'sped'] /\n",
    "     temp2.loc[temp2.date.isin(common_dates), 'tmpf'])],\n",
    "    axis=1).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have a bug in there. Can you spot it?\n",
    "I only grabbed the dates from `sped2` in the line `sped2.loc[sped2['date'].isin(common_dates), 'date']`.\n",
    "Really that should be `sped2.loc[sped2.date.isin(common_dates)] | temp2.loc[temp2.date.isin(common_dates)]`.\n",
    "But I think leaving the buggy version states my case even more strongly. The `temp / sped` version where pandas aligns everything is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "There are two ways of merging DataFrames / Series in pandas.\n",
    "\n",
    "1. Relational Database style with `pd.merge`\n",
    "2. Array style with `pd.concat`\n",
    "\n",
    "Personally, I think in terms of the `concat` style.\n",
    "I learned pandas before I ever really used SQL, so it comes more naturally to me I suppose.\n",
    "\n",
    "### Concat Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat([temp, sped], axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `axis` parameter controls how the data should be stacked, `0` for vertically, `1` for horizontally.\n",
    "The `join` parameter controls the merge behavior on the shared axis, (the Index for `axis=1`). By default it's like a union of the two indexes, or an outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.concat([temp, sped], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Version\n",
    "\n",
    "Since we're joining by index here the merge version is quite similar.\n",
    "We'll see an example later of a one-to-many join where the two differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True,\n",
    "         how='outer').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like I said, I typically prefer `concat` to `merge`.\n",
    "The exception here is one-to-many type joins. Let's walk through one of those,\n",
    "where we join the flight data to the weather data.\n",
    "To focus just on the merge, we'll aggregate hour weather data to be daily, rather than trying to find the closest recorded weather observation to each departure (you could do that, but it's not the focus right now). We'll then join the one `(airport, date)` record to the many `(airport, date, flight)` records.\n",
    "\n",
    "Quick tangent, to get the weather data to daily frequency, we'll need to resample (more on that in the timeseries section). The resample essentially splits the recorded values into daily buckets and computes the aggregation function on each bucket. The only wrinkle is that we have to resample *by station*, so we'll use the `pd.TimeGrouper` helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_cols = ['unique_carrier', 'origin', 'dest', 'tail_num', 'fl_num', 'fl_date']\n",
    "data_cols = ['crs_dep_time', 'dep_delay', 'crs_arr_time', 'arr_delay',\n",
    "             'taxi_out', 'taxi_in', 'wheels_off', 'wheels_on']\n",
    "\n",
    "df = flights.set_index(idx_cols)[data_cols].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mode(x):\n",
    "    '''\n",
    "    Arbitrarily break ties.\n",
    "    '''\n",
    "    return x.value_counts().index[0]\n",
    "\n",
    "aggfuncs = {'tmpf': 'mean', 'relh': 'mean',\n",
    "            'sped': 'mean', 'mslp': 'mean',\n",
    "            'p01i': 'mean', 'vsby': 'mean',\n",
    "            'gust_mph': 'mean', 'skyc1': mode,\n",
    "            'skyc2': mode, 'skyc3': mode}\n",
    "# TimeGrouper works on a DatetimeIndex, so we move `station` to the\n",
    "# columns and then groupby it as well.\n",
    "daily = (weather.reset_index(level=\"station\")\n",
    "                .groupby([pd.TimeGrouper('1d'), \"station\"])\n",
    "                .agg(aggfuncs))\n",
    "\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have daily flight and weather data, we can merge.\n",
    "We'll use the `on` keyword to indicate the columns we'll merge on (this is like a `USING (...)` SQL statement), we just have to make sure the names align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flights.fl_date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flights.origin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily.reset_index().date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily.reset_index().station.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The merge version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily_ = (\n",
    "    daily\n",
    "    .reset_index()\n",
    "    .rename(columns={'date': 'fl_date', 'station': 'origin'})\n",
    "    .assign(origin=lambda x: pd.Categorical(x.origin,\n",
    "                                            categories=flights.origin.cat.categories))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flights.fl_date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily.reset_index().date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = pd.merge(flights, daily_,\n",
    "             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n",
    "\n",
    "m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data-wrangling on its own is never the goal, let's do some quick analysis.\n",
    "Seaborn makes it easy to explore bivariate relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the various [sky coverage states](https://en.wikipedia.org/wiki/METAR#Cloud_reporting):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.groupby('skyc1').dep_delay.agg(['mean', 'count']).sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels (via [patsy](http://patsy.readthedocs.org/) can automatically convert dummy data to dummy variables in a formula with the `C` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod = sm.OLS.from_formula('dep_delay ~ C(skyc1) + tmpf + relh + sped + mslp', data=m)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(res.fittedvalues, res.resid, color='k', marker='.', alpha=.25)\n",
    "ax.set(xlabel='Predicted', ylabel='Residual')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those residuals should look like white noise.\n",
    "Looks like our linear model isn't flexible enough to model the delays,\n",
    "but I think that's enough for now.\n",
    "\n",
    "---\n",
    "\n",
    "We'll talk more about indexes in the Tidy Data and Reshaping section.\n",
    "[Let me know](http://twitter.com/tomaugspurger) if you have any feedback.\n",
    "Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

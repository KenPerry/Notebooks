{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal character-level Vanilla RNN model.\n",
    "\n",
    "RNN stand for \"Recurent Neural Network\".  \n",
    "To understand why RNN are so hot you _must_ read [this](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)!  \n",
    "\n",
    "This notebook to explain the _[Minimal character-level Vanilla RNN model](https://gist.github.com/karpathy/d4dee566867f8291f086)_ written by __Andrej Karpathy__  \n",
    "This code create a RNN to generate a text, char after char, by learning char after char from a textfile.\n",
    "\n",
    "I love this _character-level Vanilla RNN_ code because it doesn't use any library except numpy.\n",
    "All the NN magic in 112 lines of code, no need to understand any dependency. Everything is there! I'll try to explain in detail every line of it. Disclamer: I still need to use some external links for reference.  \n",
    "\n",
    "This notebook is for real beginners who whant to understand RNN concept by reading code.  \n",
    "Feedback welcome __@dh7net__\n",
    " \n",
    "## Let's start!  \n",
    "Let's see the original code and the results for the first 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-4858eb5f2e1e>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4858eb5f2e1e>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    print 'data has %d characters, %d unique.' % (data_size, vocab_size)\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('methamorphosis.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in xrange(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while n<=1000: # was while True: in original code\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not a NN expert, the code is not easy to understand.  \n",
    "\n",
    "If you look to the results you can see that the code iterate 1000 time, calculate a __loss__ that decrease over time, and output some text each 100 itaration.\n",
    "The output from the first iteration looks random.  \n",
    "After 1000 iterations, the NN is able to create words that have plausible size, don't use too much caps, and can create correct small words like \"the\", \"they\", \"be\", \"to\".  \n",
    "If you let the code learn over a nigth the NN will be able to create almost correct sentences:  \n",
    "_\"with home to get there was much hadinge everything and he could that ho women this tending applear space\"_  \n",
    "This is just a simple exemple, and there is no doubt this code can do much better.\n",
    "\n",
    "## Theorie\n",
    "This code build a neural network that is able to predict one char from the previous one.  \n",
    "In this example, it learn from a text file, so he can learn words and sentence ; if you feed HTML or XML during the tranning it can produce valid HTML or XML sequences.  \n",
    "At each step it can use some results from the previous step to keep in memory what is going on.  \n",
    "For instance if the previous char are \"hello worl\" the model can guess that the next char is \"d\".\n",
    "\n",
    "This model contain parameters that are initialized randomly and the trainning phase try to find optimal values for each of them. \n",
    "During the trainning process we do a _\"gradient descent\"_:\n",
    "* We give to the model a pair of char: the input char and the target char. The target char is the char the network should guess, it is the next char in our trainning text file.\n",
    "* We calculate the probability for every possible next char according to the state of the model, using the paramters (This is the forward pass).\n",
    "* We create a distance (the loss) between the previous probabilty and the target char.\n",
    "* We calculate gradients for each of our parameters to see witch impact they have on the loss. (A fast way to calculate all gradients is called the backward pass).\n",
    "* We update all parameters in the direction that help to minimise the loss\n",
    "* We iterate until their is no more progress and print a generated sentence from times to times.\n",
    "\n",
    "# Let's dive in! \n",
    "\n",
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * encode char into vectors\n",
    "* Define the Network\n",
    "* Define a function to create sentences from the model\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradiend and update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "Let's have a closer look to every line of the code.  \n",
    "__Disclaimer:__ the following code is cut and pasted from the original ones, with some adaptation to make it clearer for this notebook, like adding some _print_.\n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "For this example, I used Methamorphosis from Kafka (Public Domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"                                                                                                                                                                                           \n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)                                                                                                             \n",
    "BSD License                                                                                                                                                                                   \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O                                                                                                                                                                                    \n",
    "data = open('methamorphosis.txt', 'r').read() # should be simple plain text file   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "Neural networks can only works on vectors. (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "For this we count the number of unique char (*vocab_size*). It will be the size of the vector. \n",
    "The vector contain only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### First we calculate *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 119163 characters, 61 unique.\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we create 2 dictionary to encode and decode a char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, '!': 1, ' ': 2, '\"': 3, \"'\": 4, ')': 5, '(': 6, '-': 7, ',': 8, '.': 9, ';': 10, ':': 11, '?': 12, 'A': 13, 'C': 14, 'B': 15, 'E': 16, 'D': 17, 'G': 18, 'F': 19, 'I': 20, 'H': 21, 'J': 22, 'M': 23, 'L': 24, 'O': 25, 'N': 26, 'Q': 27, 'P': 28, 'S': 29, 'U': 30, 'T': 31, 'W': 32, 'V': 33, 'Y': 34, 'a': 35, 'c': 36, 'b': 37, 'e': 38, 'd': 39, 'g': 40, 'f': 41, 'i': 42, 'h': 43, 'k': 44, 'j': 45, 'm': 46, 'l': 47, 'o': 48, 'n': 49, 'q': 50, 'p': 51, 's': 52, 'r': 53, 'u': 54, 't': 55, 'w': 56, 'v': 57, 'y': 58, 'x': 59, 'z': 60}\n",
      "{0: '\\n', 1: '!', 2: ' ', 3: '\"', 4: \"'\", 5: ')', 6: '(', 7: '-', 8: ',', 9: '.', 10: ';', 11: ':', 12: '?', 13: 'A', 14: 'C', 15: 'B', 16: 'E', 17: 'D', 18: 'G', 19: 'F', 20: 'I', 21: 'H', 22: 'J', 23: 'M', 24: 'L', 25: 'O', 26: 'N', 27: 'Q', 28: 'P', 29: 'S', 30: 'U', 31: 'T', 32: 'W', 33: 'V', 34: 'Y', 35: 'a', 36: 'c', 37: 'b', 38: 'e', 39: 'd', 40: 'g', 41: 'f', 42: 'i', 43: 'h', 44: 'k', 45: 'j', 46: 'm', 47: 'l', 48: 'o', 49: 'n', 50: 'q', 51: 'p', 52: 's', 53: 'r', 54: 'u', 55: 't', 56: 'w', 57: 'v', 58: 'y', 59: 'x', 60: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print char_to_ix\n",
    "print ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allow us to create a vector of size 61 instead of 256.  \n",
    "Here and exemple for char 'a'  \n",
    "The vector contains only zero, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "#print vector_for_char_a\n",
    "print vector_for_char_a.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters                                                                                                                                                                             \n",
    "hidden_size = 100 # size of hidden layer of neurons                                                                                                                                           \n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wxh contain 6100 parameters\n",
      "Whh contain 10000 parameters\n",
      "Why contain 6100 parameters\n",
      "bh contain 100 parameters\n",
      "by contain 61 parameters\n"
     ]
    }
   ],
   "source": [
    "# model parameters                                                                                                                                                                            \n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "print 'Wxh contain', Wxh.size, 'parameters'\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "print 'Whh contain', Whh.size, 'parameters'\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output    \n",
    "print 'Why contain', Why.size, 'parameters'\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "print 'bh contain', bh.size, 'parameters'\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "print 'by contain', by.size, 'parameters'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 'Jm;wkbHoVjd.AjGoOM:B (aNpGE!thWPmrJH.H.AEpuxmUp\n",
      ";ESQq\n",
      "apDrOdHFpS QIqcabjnECM)JgoltDn!z;BuQmLcDGyBtQVNh!Bi;OYM.O!VdEF? TcJIqH?UrBL?)cUghP,jM\n",
      "m(sV!?':f(qt l;!aF-lY:q:kN-pf)w\n",
      "vcU)Txg,UDro\"FQFqaWbEwaSw(B \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step                                                                                                                               \n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "The __loss__ is a key concept in all neural networks trainning. \n",
    "It is a value that describe how bag/good is our model.  \n",
    "It is always positive, the closest to zero, the better is our model.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the trainning phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculate the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the trainning set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculate the backward pass to calculate the gradients (see the backword pass paragraph) \n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function output:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state\n",
    "\n",
    "Here the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in xrange(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation                                                                                                                        \n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards                                                                                                                                          \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y                                                                                                                                                     \n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
    "\n",
    "xs[t] is the vector that encode the char at position t\n",
    "ps[t] is the probabilities for next char\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "```\n",
    "\n",
    "or is dirty pseudo code for each char\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
    "ys = hs*Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "\n",
    "To dive into the code, we'll work on one char only (we set t=0 ; instead of the \"for each\" loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(\n",
      ")= 0.0164 \n",
      "p(!)= 0.0164  p( )= 0.0164  p(\")= 0.0164  p(')= 0.0164  p())= 0.0164  p(()= 0.0164  p(-)= 0.0164 \n",
      "p(,)= 0.0164  p(.)= 0.0164  p(;)= 0.0164  p(:)= 0.0164  p(?)= 0.0164  p(A)= 0.0164  p(C)= 0.0164 \n",
      "p(B)= 0.0164  p(E)= 0.0164  p(D)= 0.0164  p(G)= 0.0164  p(F)= 0.0164  p(I)= 0.0164  p(H)= 0.0164 \n",
      "p(J)= 0.0164  p(M)= 0.0164  p(L)= 0.0164  p(O)= 0.0164  p(N)= 0.0164  p(Q)= 0.0164  p(P)= 0.0164 \n",
      "p(S)= 0.0164  p(U)= 0.0164  p(T)= 0.0164  p(W)= 0.0164  p(V)= 0.0164  p(Y)= 0.0164  p(a)= 0.0164 \n",
      "p(c)= 0.0164  p(b)= 0.0164  p(e)= 0.0164  p(d)= 0.0164  p(g)= 0.0164  p(f)= 0.0164  p(i)= 0.0164 \n",
      "p(h)= 0.0164  p(k)= 0.0164  p(j)= 0.0164  p(m)= 0.0164  p(l)= 0.0164  p(o)= 0.0164  p(n)= 0.0164 \n",
      "p(q)= 0.0164  p(p)= 0.0164  p(s)= 0.0164  p(r)= 0.0164  p(u)= 0.0164  p(t)= 0.0164  p(w)= 0.0164 \n",
      "p(v)= 0.0164  p(y)= 0.0164  p(x)= 0.0164  p(z)= 0.0164 \n",
      "Next char code is: 4\n",
      "Next char is: '\n"
     ]
    }
   ],
   "source": [
    "# uncomment the print to get some details\n",
    "xs, hs, ys, ps = {}, {}, {}, {}\n",
    "hs[-1] = np.copy(hprev)\n",
    "# forward pass                                                                                                                                                                              \n",
    "t=0 # for t in xrange(len(inputs)):\n",
    "xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "xs[t][inputs[t]] = 1 \n",
    "# print xs[t]\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state \n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "# print ys[t]\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars  \n",
    "# print ps[t].ravel()\n",
    "\n",
    "# Let's build a dict to see witch probablity is associated with witch char\n",
    "probability_per_char =  { ch:ps[t].ravel()[i] for i,ch in enumerate(chars) }\n",
    "# uncoment the next line to see the raw result\n",
    "# print probability_per_char\n",
    "\n",
    "# To print the probability in a way that is more easy to read.\n",
    "for x in range(vocab_size):\n",
    "    print 'p('+ ix_to_char[x] + \")=\", \"%.4f\" % ps[t].ravel()[x],\n",
    "    if (x%7==0):\n",
    "        print \"\"\n",
    "    else:\n",
    "        print \"\",\n",
    "        \n",
    "# We can create the next char from the above distribution\n",
    "ix = np.random.choice(range(vocab_size), p=ps[t].ravel())\n",
    "print\n",
    "print \"Next char code is:\", ix\n",
    "print \"Next char is:\", ix_to_char[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the previous code several time. A probability is generated during each iteration.  \n",
    "\n",
    "### Loss\n",
    "For each char in the input the forward pass calculate the probability of the next char  \n",
    "The loss is the sum \n",
    "```python\n",
    "loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "```\n",
    "\n",
    "The loss is calculate using Softmax. [more info here](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/) and [here](https://en.wikipedia.org/wiki/Softmax_function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next char from training (target) was number 36 witch is \"c\"\n",
      "Probability for this letter was 0.0164086939046\n",
      "loss for this input&target pair is 4.10994396848\n"
     ]
    }
   ],
   "source": [
    "print 'Next char from training (target) was number', targets[t], 'witch is \"' + ix_to_char[targets[t]] + '\"'\n",
    "print 'Probability for this letter was', ps[t][targets[t],0]\n",
    "\n",
    "loss = -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "print 'loss for this input&target pair is', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but would be time consuming.\n",
    "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "```\n",
    "\n",
    "This part need more work to explain the code, but __[here](http://karpathy.github.io/neuralnets/) a great source to understand this technic in detail.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# backward pass: compute gradients going backwards                                                                                                                                          \n",
    "dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "dhnext = np.zeros_like(hs[0])\n",
    "t=0 #for t in reversed(xrange(len(inputs))):\n",
    "dy = np.copy(ps[t])\n",
    "dy[targets[t]] -= 1 # backprop into y   \n",
    "#print dy.ravel()\n",
    "dWhy += np.dot(dy, hs[t].T)\n",
    "#print dWhy.ravel()\n",
    "dby += dy\n",
    "#print dby.ravel()\n",
    "dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "dbh += dhraw\n",
    "dWxh += np.dot(dhraw, xs[t].T)\n",
    "dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "dhnext = np.dot(Whh.T, dhraw)\n",
    "for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "  np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  #print dparam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "This last part of the code is the main trainning loop:\n",
    "* Feed the network with portion of the file. Size of cunck is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradiens\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradien technique Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [25, 49, 38, 2, 46, 48, 53, 49, 42, 49, 40, 8, 2, 56, 43, 38, 49, 2, 18, 53, 38, 40, 48, 53, 2]\n",
      "targets [49, 38, 2, 46, 48, 53, 49, 42, 49, 40, 8, 2, 56, 43, 38, 49, 2, 18, 53, 38, 40, 48, 53, 2, 29]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print \"inputs\", inputs\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print \"targets\", targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 102.771849\n",
      "----\n",
      " JbjYSzj'm).BgJJ;!)u-s( Ep?WhtV'ha.A?G)w?sEuAF)yGW':zVchYhpTICMJliphLk.\n",
      "BM'B:wHJCqFA-xlD\"!AsiTrk-LVwzlL)UDozcA;zPGs:LASLVpon\"-uqWdTWL!hOY?cMWDirCEJAGPD!lqG\n",
      "jyqAcek!P S\n",
      "ox jymQ;yi  A!dfzWSIoJkdsChnyTl!p \n",
      "----\n",
      "iter 1000, loss: 82.722818\n",
      "----\n",
      " uos pes\n",
      "osg- hon ifrd whim bln sfate has asungin morf wen, wus tvee irs tot g ocon, pou theet wit\n",
      "h ftor  \"avy nitle nean.  wasl plee wigfe tt ole le smer, orok, thang xTapst thet tlongt, hou beet, it \n",
      "----\n",
      "iter 2000, loss: 67.465023\n",
      "----\n",
      "  dool. cgihe sheasse thing at thir ll hos shasitten beth the hot herhin\n",
      "tho, the nit thet thar heat he he hasewtow\n",
      "hamges poore\n",
      "ssik ho merkomeat Greld o hor thos t heawye bome\n",
      "Iof hat stuuhy meres as \n",
      "----\n",
      "iter 3000, loss: 59.508360\n",
      "----\n",
      " es wast; he he homed whrrey whanls the duong at wamt ristey hterald wald andrtowhs hi hy stiredt and -olf llangurger shis tudod thens why cio nad the fnoll ag ht corout  has come\n",
      "wut wave wor re his n \n",
      "----\n",
      "iter 4000, loss: 55.673855\n",
      "----\n",
      " e tosceing por to trerre thermoule, way, wore om as ould thefreror boow int, andut it meft; ard ackean dow.  Hum\n",
      "fimwtimford\n",
      "thitiias meary\n",
      "Grengittimmallt haster gat his winimcorser for ancamed fre't \n",
      "----\n",
      "iter 5000, loss: 54.816958\n",
      "----\n",
      " flor and hat kut the wat, he drrssiser mome\n",
      "onhint thercif\n",
      "and to Gregor he te ste\n",
      "cet asclouls heak\n",
      "a stisp Her and his begerpron's oull wis thay bey won anthe plot thed mini math tor. 'ove\n",
      "waif ea d \n",
      "----\n",
      "iter 6000, loss: 53.754304\n",
      "----\n",
      "  sors amt?\n",
      "wartlend wem evind hivtlaped that he\n",
      "way\n",
      "seel nom eel of the hith waint and and\n",
      "couliss whakkeg.  \"Jo the; slkenss ug\n",
      "it axv ifpifh ora ro mioo\n",
      "cout arpave, colgs, strenting her asdig st ar \n",
      "----\n",
      "iter 7000, loss: 51.183773\n",
      "----\n",
      " hes thenrege tay wheut way was\n",
      "had marentent of and nor long thith threistient, and enche fly weraweverr wis the waofed sold was ont tould  pral and it bey theven to past. aFone\n",
      "beed ittre oo licm bse \n",
      "----\n",
      "iter 8000, loss: 50.043872\n",
      "----\n",
      " ted if of the frefde of his be thonef\n",
      "ad\n",
      "to frow's cister at ung way poute ss yo he laded sleen Gregovers\n",
      "thein\n",
      "if abatiely ffen his coug his futhor's of hish, to amkove was gnt the waus itherkomred t \n",
      "----\n",
      "iter 9000, loss: 49.347803\n",
      "----\n",
      " s aFded rirplosous, as of\n",
      "him upslother\n",
      "beat That ded to her or the herrit't fore)\n",
      "Grdedr lave om\n",
      "the nas jorsile a wantaked oO Gregor'm this\n",
      "s apfistinge his jular sims mored,\n",
      "beot whough himure her  \n",
      "----\n",
      "iter 10000, loss: 49.835265\n",
      "----\n",
      "  thow?\" he led sersy teay if w!er to aill; to thes of\n",
      "tout\n",
      "trerere, he hap quget beiter nent betith fuld he fom expeligh creatent of?\", s meftors, ble thinh he chen he\n",
      "he heve\n",
      "tid ins begor hes moor i \n",
      "----\n",
      "iter 11000, loss: 49.225069\n",
      "----\n",
      " ve to on\n",
      "ullstn at inly wich of on rop caingo was cout;\n",
      "cole wands wasting, ar sask at ats; pas\n",
      "apl ando his hepratittout him thoughs t raso.  Ondithe the werthad als amshingow simas ont, ctwokean\n",
      "fai \n",
      "----\n",
      "iter 12000, loss: 47.582613\n",
      "----\n",
      " e, would jherning llisted; he conlery agque the not momess Greto he wive earaint as alt with\n",
      "noulm sol there wher, could a\n",
      "Grexpure the sollser, futpentinn He tough the ont reend thing thar bocomerot  \n",
      "----\n",
      "iter 13000, loss: 47.146045\n",
      "----\n",
      " e on ipptld all frenod aromcren sime, morter, at uther.  Smoking mothathler beate bunt muck woull would she er, stoom fhe allecond goll, she fatapetime wammed neme purhen sime stoid deling enen moter  \n",
      "----\n",
      "iter 14000, loss: 46.993106\n",
      "----\n",
      " lomv, shabled litent airssist\n",
      "wempuind cothed.  He sats.  The lood on't on heaslaumabitlomed so shing ule llenir who ever, gempor in Gregary. Sur Chr) She crenastlry,\n",
      "at the beimtitiod and the cloom t \n",
      "----\n",
      "iter 15000, loss: 47.551772\n",
      "----\n",
      " nd the chter, llanduvi's coat dit'ling his sliing growed in's nove forlot ses the, ever for his wour Sherre st in sce coth it a cellpawasa by age, bemind. \"\", was yomed it.  Bo s rennly tile bus souge \n",
      "----\n",
      "iter 16000, loss: 46.670208\n",
      "----\n",
      " ing whis\n",
      "in\n",
      "thipe, sert woom\n",
      "sister intif to nimpeap of hid\n",
      "of aitce in forn, authing limmen furiththing sor\n",
      "this be thhe sapliine thet wentting.  Shat hak beed of ligholy live duther dilk fith's atar \n",
      "----\n",
      "iter 17000, loss: 45.708633\n",
      "----\n",
      " d wing woucef to surting\n",
      "that haw,\n",
      "he waull his\n",
      "cly not, hod meding comed ho\n",
      "wound the bed herwey\n",
      "ot the wat; he's, and apludl. ever, whut be.  nat quided that bresen thers\n",
      "to ste verk abomed shen elj \n",
      "----\n",
      "iter 18000, loss: 45.409084\n",
      "----\n",
      " omen har to to  lats hid will a diof ary, she time\n",
      "he\n",
      "thound to Gregor to himmef nothan ous a rothed mowt: fut wemp, whot whut lime all and\n",
      "evered he waud doove, awonous in they hiver thein would Greg \n",
      "----\n",
      "iter 19000, loss: 45.916607\n",
      "----\n",
      " urad to le thone sauys tratsisted got he wamped\n",
      "and,\n",
      "not forny a msain cool sor torywire shea\n",
      "clolthound; opsor rother bewirgse\n",
      "wand he lids\n",
      "entorged suckfftartiled rime look was. laYked beangile\n",
      "ont  \n",
      "----\n",
      "iter 20000, loss: 46.777101\n",
      "----\n",
      " hise at wo talliring to he jutlithelf\n",
      "sistoong not ank the fet ithen whand allit I wos astens, lloples go xisfpstiln\n",
      "fo sCeen\n",
      "stayt whencoot hit best would wiolghercs, it not ont kerser youf desting t \n",
      "----\n",
      "iter 21000, loss: 45.416953\n",
      "----\n",
      "  the wistult -othing it\n",
      "onen statoin thaw of chisselly\n",
      "nem;\n",
      "it thanguricesh teabled whate erily en'th spreting thow unet snoor, of wise on hom it way so lever fathimgoat with wad sowt havred buck to h \n",
      "----\n",
      "iter 22000, loss: 44.553136\n",
      "----\n",
      " oas,\n",
      "spst though wonl, whussecs; ofl there wasulith\n",
      "flome, cereed he to neventure ouht would coacrued ady\n",
      "evely steno sa whoughot she har sast'nt tule had wast out a any\n",
      "wat\n",
      "bave, Ont; mvonchom, where \n",
      "----\n",
      "iter 23000, loss: 44.448019\n",
      "----\n",
      " r stimubpenen not muss; the fleas a diotingean stace but of casyeald and weris\n",
      "in mothered and wat the betoat.  as theas tobe thems pistoing bestell, to\n",
      "caime\n",
      "cald notint thoubole.  Bute and the roin  \n",
      "----\n",
      "iter 24000, loss: 45.587333\n",
      "----\n",
      " les have buce's seat jus.  He vee ort\n",
      "samp evenont waste there beel.  He tilpith, outsing. Sa viens as them of not to the lun so fess a hard desss, one somens not had garked.  Htc out fated deerto the \n",
      "----\n",
      "iter 25000, loss: 45.743944\n",
      "----\n",
      " Mr thill, the efprice floable hot in thaighticghe steffacl his meat ser fors hir haind ther and weper lowh the kagcen gatingule!\n",
      "Thtid so came corse roull the wrars stound \"fis - wauld dein porably ll \n",
      "----\n",
      "iter 26000, loss: 44.414475\n",
      "----\n",
      " 's he cold a must athed his gount\n",
      "and\n",
      "serse!d he cherking, home tad\n",
      "was soo had ontentter, had nege that righiss theme a was timets beel wat they whe gloull hes har dimntetseait\n",
      "allut condeatein was w \n",
      "----\n",
      "iter 27000, loss: 44.100444\n",
      "----\n",
      " d in als the lood had ssiough pere futhiet?\"\", room, him.  Wemssar, and is.\n",
      "\"Ont intess, at hourt oullily mare the ble on thy aforing his his ever was erentally key and through lystay evenoth, sheverr \n",
      "----\n",
      "iter 28000, loss: 43.883808\n",
      "----\n",
      " at in his roor it in allood there seed geat his fornedhed\n",
      "a to the llinghiig\n",
      "was rust.  Her wood pidcels herd Cot hecH was and\n",
      "elle\n",
      "he had to every not\n",
      "whechs.  \"What, Gregor's oven he he corkoid cay  \n",
      "----\n",
      "iter 29000, loss: 44.864125\n",
      "----\n",
      " en he wircinet fist; out whol ous\n",
      "a out uple had ifthen hive at come afarabe,\n",
      "his he haip.  \"Y's\n",
      "wistelf shaine Threak ont\n",
      "beding to hand, a the cond had dessatuont not bedive hit nhom weed jusher wen \n",
      "----\n",
      "iter 30000, loss: 44.754627\n",
      "----\n",
      "  haddon\n",
      "there to Graimien pacting sees was a whilqulcick he wat led yout hiwer for the\n",
      "prome-t timasly pnomute whare prodeat\n",
      "being and\n",
      "and sadree furs in his\n",
      "astight do's wight!\", and they him sasthad \n",
      "----\n",
      "iter 31000, loss: 43.504488\n",
      "----\n",
      " ence ther bethe he\n",
      "from the Cas neatt, erimus deas, ofenasessed Mryemed to te ow.  She culled mave louse himwather has to wincould she ske way stowant, veit and uledly, he mee\n",
      "simerally bees no.  op h \n",
      "----\n",
      "iter 32000, loss: 43.484882\n",
      "----\n",
      " reet mook even bethe frot was that the beat see then ther wonct rimsliet to compegne of everyhat he could move for, to hend and the wath on was shee corchible enous bechese thin in bod the of could as \n",
      "----\n",
      "iter 33000, loss: 43.390059\n",
      "----\n",
      " oreatiln sposing atile the  Buthet the caitious serner him thas thes Gregor do hands slowound sead,\n",
      "Gregor's\n",
      "st but out, lyooucl couclly in he would fhem erbeary whe\n",
      "larpe it line for.\n",
      "y\n",
      "dom.  \"It lok \n",
      "----\n",
      "iter 34000, loss: 44.082598\n",
      "----\n",
      " es buliss and and cimsly not gout in his than for furning\n",
      "lave wast was stown han's his move tliding Grego his in the?  Mes the sime in that 's uremorely\n",
      "undion, tow to liffen, doo hard hair and the s \n",
      "----\n",
      "iter 35000, loss: 43.709285\n",
      "----\n",
      " me at in onf hid amacet whis about - the atine sowing in it.  Heon she queling unkegsidg good hard; feacked frree thalice forgst lying not holmint breain, and his make to eas mate not a was in though, \n",
      "----\n",
      "iter 36000, loss: 42.765192\n",
      "----\n",
      " bly of cleckart yould ard heres, anght firte at undoweblbent\n",
      "it chack ention\n",
      "coulo puling that last\n",
      "under what whis in wime; the from intay samce.\n",
      "for'd\n",
      "GreNovo; was noibly pollen onen\n",
      "himsly\n",
      "would go \n",
      "----\n",
      "iter 37000, loss: 42.658847\n",
      "----\n",
      " ven lently.  Hand than's to his simating hingrenen to her, the  He\n",
      "way ppefoot of in indo bens didcels\n",
      "wime of a drom, rwingeaked, turder sothoter, and the cive\n",
      "eacmad into Gregow theren any lave, the \n",
      "----\n",
      "iter 38000, loss: 43.009800\n",
      "----\n",
      "  to as itid tnated nous a pily entomee aboul herme a mutter to llet the dartertore and this sMre afle wern acled weppew of the dar stoory here, say.\n",
      "\n",
      "I\n",
      "any wistladsarreis rea.  The kay lpageabded deaw \n",
      "----\n",
      "iter 39000, loss: 44.123542\n",
      "----\n",
      " hinnt on himsind, for shemment his sid in the wak fowry wand takin with a wele\n",
      "Gregor's of of he his tough to I whan stay and not tive hay as in with he be the singous critsing, what whe play\n",
      "exe to b \n",
      "----\n",
      "iter 40000, loss: 43.144511\n",
      "----\n",
      " ld though ad as at the capputed in himsel the garisele weyh ont Gregor's the his fay a mubiny bving thher rasthing had thace, feet shosing\n",
      "whach.\"  \"Juve so dicreamree thared at of mus all frould west \n",
      "----\n",
      "iter 41000, loss: 42.306902\n",
      "----\n",
      " tef asayfull\n",
      "bockigens pufentelf to a hersted, over; stay one, and the minther him lalleved grely as the heen farin ig.  His coowh her\n",
      "exfe wat\n",
      "cleady;\n",
      "room move for him.\n",
      "she wonlot had stove\n",
      "har that \n",
      "----\n",
      "iter 42000, loss: 42.155369\n",
      "----\n",
      "  the deawentil the wim\n",
      "he there\n",
      "in it what pereating atleen.  some, bectusted bads ut everyed\n",
      "and the ham tay looked Onter forgey not lowere was\n",
      "no mother and bremeambeating to pleckinte, ast; to towe \n",
      "----\n",
      "iter 43000, loss: 43.117955\n",
      "----\n",
      " tome aspoilly ulare betntes to heve tuch afosungont, sis beftrerthilct enthing ampew\n",
      "in the chacr\n",
      "thoughy have, at then, \"Fold, mught about tak jood tomett and herwh as ho himselvown his littly crousa \n",
      "----\n",
      "iter 44000, loss: 43.401847\n",
      "----\n",
      " n would loom exilit, we,\n",
      "yould ofl whe kece the\n",
      "dowlyen it.  But the could harsly then the rouch then wimprinint and cowhrer'sing\n",
      "on beroos of qouth weal souped hanbid of the \"s himsert his moong it p \n",
      "----\n",
      "iter 45000, loss: 42.504677\n",
      "----\n",
      " \n",
      "ases at enefeaver the littled all him you he wied he wauch; fat hismed tood amy and himse, fore nos ule ofrresid in; so retly had ryom the omee had again must in snapsuticed deass an?, The salto- of  \n",
      "----\n",
      "iter 46000, loss: 42.142305\n",
      "----\n",
      " wiodn hiss.\n",
      " lewass allt vealised.  Wets le that poblily; was not he hear fertarde!, where bremest agaunfoled for lack eare, nowt up at crapter;\n",
      "it poen lookid befelcter of chis.  Jundion on to as ple \n",
      "----\n",
      "iter 47000, loss: 41.950116\n",
      "----\n",
      " d but was sark and thersed the wank and it was ont wimh.  Aff.  The ryomained room looked'r siscos rece geys, it this baciret of crapting it reovuch and to way or Gregor's as aware into\n",
      "be the of play \n",
      "----\n",
      "iter 48000, loss: 43.170663\n",
      "----\n",
      " .  But food colk oft whle while as his abling when.\n",
      "\n",
      "Uncelver,\n",
      "rook but at.\n",
      "\"Okturne stion murce the voor a thong would his nad all, shote, the destarn ut were or the fithed that the cast bent of they \n",
      "----\n",
      "iter 49000, loss: 43.110767\n",
      "----\n",
      " the\n",
      "bude deed\n",
      "shound to mo was when an to rAtieg, his sist rast casls, would not strover\n",
      "llith\n",
      "to Gret'r mother a lommy.\n",
      "\n",
      "\"Whe  ond gand.  The door dorgwiktiof to ar\n",
      "that, so pertibe, some has saden i \n",
      "----\n",
      "iter 50000, loss: 42.018171\n",
      "----\n",
      " l the longi, when hey, was, have\n",
      "I loff.\n",
      " Erich ot the out\n",
      "dering\n",
      "it have everver sereanteds to that furds\n",
      "Grestrettlen, he was\n",
      "fore.\n",
      " abous bicsoust buch this shout Gregor the pothurds, in the  and,  \n",
      "----\n",
      "iter 51000, loss: 41.889384\n",
      "----\n",
      " n it.\n",
      "sTheretharn'sly he day awather one\n",
      "on ancay save of wother bed taitifulr, to he hen hes wleally wore his fothisppal dy wroces\n",
      "she conlless, gheren one mocely of oflly bedaw, pareeally, were pick \n",
      "----\n",
      "iter 52000, loss: 41.771745\n",
      "----\n",
      " \n",
      "in s up tolled had\n",
      "call-y ervetadichmie the and betraine had theme grag there now, sAant som the was from to nexpeaten, Gregor, \"f\n",
      "sele\n",
      "his have to mead gening would and ant rave a\n",
      "smoundmiling the d \n",
      "----\n",
      "iter 53000, loss: 42.550489\n",
      "----\n",
      " ee\n",
      "though to an the fally a tilalo that, whis with theik and itredse of wours himsar bealced now nound turne.  \"the soow\n",
      "wam\n",
      "nog ture, would ettly and filf, hem and his morewo., alms\n",
      "ledne? Gregor\n",
      "wac \n",
      "----\n",
      "iter 54000, loss: 42.295011\n",
      "----\n",
      " ou derl, as slest -quindif himpalaced agayed only mo mees frinhing ont\n",
      "she\n",
      "hand.  I have tham's in ofhed while the choprte now frreaway the\n",
      "vions inid, a wass he hadne hard.  Thould his vensplened\n",
      "bea \n",
      "----\n",
      "iter 55000, loss: 41.315275\n",
      "----\n",
      "  that hele to let tho hens his sibegare the not be agaim ance thing agrild, Gregor to that Gregor rettiod of a bet wertednife the rith could lingen of the way to his she was wos uode, unfol only surea \n",
      "----\n",
      "iter 56000, loss: 41.369117\n",
      "----\n",
      " y iture of an therly carther, armirt overenn did sad\n",
      "of only really his storr it.\n",
      "Sars, pien betittent his pach he fling\n",
      "they deed longost, shay breagor.  He her being\n",
      "loighat beiming rarking his Greg \n",
      "----\n",
      "iter 57000, loss: 41.641125\n",
      "----\n",
      " d turter.\n",
      "Gave frothere\n",
      "they\n",
      "the comfamess\n",
      "he thain.  Whablided or the dod, intirel\n",
      "aiver cronese whallary.  fe hindeng think the shem simet the could and rugh - to the ceet fister.\n",
      "So had lone\n",
      "and bo \n",
      "----\n",
      "iter 58000, loss: 42.615627\n",
      "----\n",
      " Mry shoreww\". Sad leall with to had cupsireng beants on came veay with this moveles\n",
      "mugh of than!\", I\".  Whe head ot's into that hers.  Ablut\n",
      "bads of yomury.\n",
      "Gregor could lay, to enock on the could.   \n",
      "----\n",
      "iter 59000, loss: 41.798872\n",
      "----\n",
      " ne xlown neably the good Gregor's atosped, a whe was sertol all orenious; he\n",
      "prestabing; and deforter not bearfade, that he dive wouting, her mory evooy stoor asthan\n",
      "stoonlitgor acelverd\n",
      "he\n",
      "forsilly b \n",
      "----\n",
      "iter 60000, loss: 40.973116\n",
      "----\n",
      " ing ponlyhin\n",
      "the tive bray whather hoved the diok therr eating undersay Gregor gabunyt they was lleas, sister\n",
      "he hear himself to fornien at whis mother's, haach she prartsed; this, (notilatithock.\n",
      "\n",
      "Al \n",
      "----\n",
      "iter 61000, loss: 40.990850\n",
      "----\n",
      " ishing out down was every in the monn sost\n",
      "I'm samrearde the femped chase had away to freep; on the\n",
      "know,\n",
      "and thewar boitain to himselvering\n",
      "Gregor the bugh the maningny.  The from themes, sthered her \n",
      "----\n",
      "iter 62000, loss: 41.718641\n",
      "----\n",
      " fatsed.\n",
      "\n",
      "Feld himself if thome that hims a dig, were and now, a magdent.  He was so were to\n",
      "motion and the reicling from uprey now kfor as tooked at and cragren noun veir in had toS, it, said lens, wh \n",
      "----\n",
      "iter 63000, loss: 42.188063\n",
      "----\n",
      " nt his enow with throk forn with becany, ba\n",
      "kef next liGs.  His with therr unferet, and all at there Gregor, I crenting ot to of retil, sean a'sle had\n",
      "back this\n",
      "they as it\n",
      "enovell ons gitceep antult w \n",
      "----\n",
      "iter 64000, loss: 41.397482\n",
      "----\n",
      "  by dide, lowald whent nocted sucuthidhy and he stayedounuse towest the meadary tol soust\n",
      "was for seslared had spothable to spe more ont wether would to mid lown the deld mast thoned stut eves fat dra \n",
      "----\n",
      "iter 65000, loss: 40.927611\n",
      "----\n",
      " his mone up and, it had head\n",
      "the wister loom, a day Gresom he was no aboulieg acoust lectikgong.  \"Fiond, kays but foll to thes havilin'ld in blat,\n",
      "awo\n",
      "for meanting and whe\n",
      "hid not\n",
      "ho doom,\n",
      "in beck ho \n",
      "----\n",
      "iter 66000, loss: 40.840589\n",
      "----\n",
      " d,\n",
      "prile, of so geet, peratiotrew was himself and pleay whil, the ported he mothick movely just scooms and it\n",
      "bation bated ther ald be almant\n",
      "he quive stack.  His histebley, was furce, \"As one the doo \n",
      "----\n",
      "iter 67000, loss: 41.960031\n",
      "----\n",
      " n more samping, to them and bulle.\n",
      "\n",
      "Went to himself of the mane being for\n",
      "thing and\n",
      "she made fecthacroetormedce her's way his sacked earerssedon\", hut shough, a lock for as if them oll aroned himsesh  \n",
      "----\n",
      "iter 68000, loss: 42.123034\n",
      "----\n",
      " amall bed to lang rettark all to he pligh kiding relment ary fbow turt.\n",
      " I was whuch hold gein, awhars some\", meare and look proor. Same losked eavile where but his gow arce was it lond temie\n",
      "lleak, f \n",
      "----\n",
      "iter 69000, loss: 41.151338\n",
      "----\n",
      " ll was weme assed under\n",
      "cul\n",
      "the though, of the slearsay, littint that woulded and was fhes have out, all thenad smis\n",
      "anther he had in\n",
      "to the loes had lietly bock it her - them corensive; lething he wa \n",
      "----\n",
      "iter 70000, loss: 40.870085\n",
      "----\n",
      " ; whore to steld ppreey moar's And homen from amot had so teed him whole apptere feonce and shoppen oren the clale, his forway to , if prock.  The fumpshone and had bigelf his sest to bete werk ag aga \n",
      "----\n",
      "iter 71000, loss: 40.921425\n",
      "----\n",
      " ing aniom and\n",
      "Gregor's nectious take\n",
      "comesally lein seed with his bise lepstamel.  Gregor aid anfthing his eack wipe.  Werced she had was\n",
      "andsich handen\n",
      "the fored hav, to his her\n",
      "in an sesple to legs  \n",
      "----\n",
      "iter 72000, loss: 41.566296\n",
      "----\n",
      " own thish for he raye from beer at half juther\n",
      "moked hurrituithent thiss.  Durtaccentened riitching felso\n",
      "Gregor wam freslly to has seod there late Clomselfact a roor frot to into be openarster from i \n",
      "----\n",
      "iter 73000, loss: 41.341905\n",
      "----\n",
      " inly but the outhaist but his fareseadyed and stion anyatianing\n",
      "breake for dirver to be was digh, upstopt ar.  I workelt, with him.  Yoolow urdif it ham holr.  He\n",
      "cacelerr onter clistle\n",
      "honing itt ghi \n",
      "----\n",
      "iter 74000, loss: 40.546938\n",
      "----\n",
      " r't opened\n",
      "and had heme.  Buthed ragh que, llester ture to te he was buted awa fron in she could he he that and the deat.\n",
      ", the, shewam room in his sister with he ham somengel at like serselet; he was \n",
      "----\n",
      "iter 75000, loss: 40.596442\n",
      "----\n",
      " iw.  He it.  But wating digh they Shis swasally peady\n",
      "erested he not and the roovel the cortite at winhturber as pprinednent all them would of to and from up into thing there fifter \n",
      "she wad stell?\",  \n",
      "----\n",
      "iter 76000, loss: 40.647307\n",
      "----\n",
      " alleikn wam her furcary gases all way thearly\n",
      "ssefered its slitt bever.  \"Go, and awaund's to ofh of ille size as neck as hink aw andirgoless wouldy oratancise opne that he stlest yout her funt.  No\n",
      "w \n",
      "----\n",
      "iter 77000, loss: 41.354840\n",
      "----\n",
      " st each whought scook, patrietull, I dispress,\n",
      " So pas holdealy quipprachings asseef unfiated an ow if rest ant of aren of he meeply ur is as her star fraifle evegr to letaked as to was is meany even  \n",
      "----\n",
      "iter 78000, loss: 40.807560\n",
      "----\n",
      "  thouth in his have for his purnoon had wast bects\n",
      "looked.  I\n",
      "dot begaugh as\n",
      "for and his pillisely mont abong all to Gregor vyom did, theiry Andin, I'm I the whis?\n",
      "Bef, toome, a bunol wheen stock, Gre \n",
      "----\n",
      "iter 79000, loss: 40.169391\n",
      "----\n",
      " rear thacged seressiry dive into lomselmated ttroply would the flears they was ther forg, the kimsed back his breeverint - the castc back orf.  Mart, compes the meet with to hes\n",
      "as he coupsasilatal lo \n",
      "----\n",
      "iter 80000, loss: 40.147090\n",
      "----\n",
      " ple; work and the themis bedy or their,\n",
      "land sol the light it, keat in thacked sumed come thet sheme But helbed acked, she tent and they sthing,\n",
      "and ond the singer of it Gregor wento with bisecs, was  \n",
      "----\n",
      "iter 81000, loss: 40.614075\n",
      "----\n",
      " n stowac on\n",
      "insice ans of her quiter jeit alrsion.  The!\"\n",
      "\"Ald a himfed early amplovily peainemome they would not movering to gace of.  So hears\n",
      "saik had reast the crote lat ther the faimed\n",
      "im anyoram \n",
      "----\n",
      "iter 82000, loss: 41.458825\n",
      "----\n",
      " heeand to carpry got mave out of he coully jump\n",
      "placicely the pide and even ast and couch cleaped rard would ray the choading.\n",
      "do, in\n",
      "that their as their, any Gregor's roon't drom wat he sorsyed door  \n",
      "----\n",
      "iter 83000, loss: 40.684151\n",
      "----\n",
      " ichused; than neevem unigil out excoom, leen\n",
      "foring raight meen where that it of ally\n",
      "at\n",
      "begabess\n",
      "open jutheaming agl the strat she drigefor thiside; heel a discy groting\n",
      "that he thinged out thung kis \n",
      "----\n",
      "iter 84000, loss: 39.794981\n",
      "----\n",
      "  steather, beth of to fat off charpersan she lanch she coulded it would pare auth to his\n",
      "sindoting way he was sister's to gourco, sho heparstivier d wore moon\n",
      "searn? Gregor was stist us, though evel,  \n",
      "----\n",
      "iter 85000, loss: 40.061049\n",
      "----\n",
      " arline ont where be quaced and Gregor's fating likerer, learying ipreatendy to drar mave lent at on thougs, movorn.  I mote apse rampure closs to bay.\n",
      "\n",
      "Gided and roops\n",
      "begaceing not her movinbyed ol G \n",
      "----\n",
      "iter 86000, loss: 41.145244\n",
      "----\n",
      " reands in caitevise was befary polligely at with them arned it fentioully that's neven seain\n",
      "sight up everyture.  Flrass back ease elpmala han toming or pees a brtinitinelarown that sid morn begom to  \n",
      "----\n",
      "iter 87000, loss: 41.452128\n",
      "----\n",
      " ers tike bun\n",
      "gony in his thainds.\n",
      "\n",
      "Gregor doom mack indeafe roor himself azings\n",
      "anyom they thair to hem would way day.\n",
      "\n",
      "The flitt tory, wiclis next to could!\"\n",
      "Gregor dowan mode very I'm sais clome any \n",
      "----\n",
      "iter 88000, loss: 40.320113\n",
      "----\n",
      " ardo\n",
      "thon's father monn the flay chat thowert the deakn\", a pethilly in of his Gregor's might came of retrother as fere naiting to crien butt.  Somy chame canearns, a whare boving a move rindow, he co \n",
      "----\n",
      "iter 89000, loss: 40.169107\n",
      "----\n",
      " dably\n",
      "ouchied do have go were could to sitted over\n",
      "for, on they hap to meft had\n",
      "encelenilly invicest one wive ert delarised it,\n",
      "shough afoundinghing atay, Gregor\n",
      "juster to frow to to his,\n",
      "it of aing.  \n",
      "----\n",
      "iter 90000, loss: 40.198080\n",
      "----\n",
      " over it wand.  Bucheing\n",
      "and up need fary forwayt or nousise so fome floorged - thisk with stlungeased to streld, being\n",
      "the\n",
      "gom. \n",
      "Even of horecead callen begairs, about very that Gretist of the sistail \n",
      "----\n",
      "iter 91000, loss: 40.960236\n",
      "----\n",
      "  \"Mrwo tratignt of he he\n",
      "so vight fuld the bath\n",
      "things lotere bically whan Think a that\n",
      "bo that of the\n",
      "room.\n",
      "\"Henk betor be\n",
      "thock, I\n",
      "driep ganon of thens befoug seid of abous oren thinh he did one one \n",
      "----\n",
      "iter 92000, loss: 40.801151\n",
      "----\n",
      "  as in alr the was out his ligh.  He had ritcroviousice thing, furned being as hack anding roungood that said lirgione.  He was wepprest called ont inttiscried casply betase s\n",
      "up of who rist of mears  \n",
      "----\n",
      "iter 93000, loss: 39.933099\n",
      "----\n",
      " d to thing the I voors sinesing tay.\n",
      "The go frentiearely\n",
      "of the greefant wascse pigrise to think not to been awathe bearef and the\n",
      "littleds.\n",
      "He matlied\n",
      "ivy alreamy war\n",
      "a demsetry towad; ask in on thin \n",
      "----\n",
      "iter 94000, loss: 40.038111\n",
      "----\n",
      " t wetke, sow, ranly appnopever, as the wake a ramcening fom help; the being it way ont, all his mare from hingen would fith even ut of they whensp, nettick,\n",
      "lack to mote comming on themreen for.\n",
      " Some \n",
      "----\n",
      "iter 95000, loss: 39.987026\n",
      "----\n",
      " not eading lith the lon in the cad in\n",
      "and that\n",
      "shought had sive shom, at coight shoug, no- to get lover armanded allisally it happed not and the windoursed he rick to have slown aetre his cime ray and \n",
      "----\n",
      "iter 96000, loss: 40.740518\n",
      "----\n",
      " oatesly would futher of they,\n",
      "the ryich frro himself and welah its he lied as at sCoos un on littoction, shey harder would bew in\n",
      "himself oten himsbon\n",
      "gass not betoking leanmed to mote,\n",
      "all him.  At c \n",
      "----\n",
      "iter 97000, loss: 40.164793\n",
      "----\n",
      "  she way it was shough thing the cared his mobe the there.  Ptrese hol forfes hoped inded\n",
      "to the way heir thas if the and to now her sumuciblen to day whalle even expelle\n",
      "botele and their him.  Heon d \n",
      "----\n",
      "iter 98000, loss: 39.709341\n",
      "----\n",
      " hive strat him whung prong as the courdings purxuling and idray leie was mith, camrent him\n",
      "beceass tha sontion\n",
      "and bell the flow\", day be to\n",
      "plans.  She into.  She door, and wanted to\n",
      "was exclisting i \n",
      "----\n",
      "iter 99000, loss: 39.646680\n",
      "----\n",
      " t.\n",
      "\n",
      "Weak, in its corentady, towtul uprey\n",
      "brong streake of the fland; hald on the firche fance he his father toalle who hisherrk wasing stread butt then she had lack keppress Gretor were out as aspally \n",
      "----\n",
      "iter 100000, loss: 39.917256\n",
      "----\n",
      " d?\", herself.  He whillquresto\", Gregor's, they in chied ore shey heartly.  On's and so dhe could down\n",
      "she wast haminabening not uble to then at ealped that the wighturle vike; the\n",
      "thightly abn overed \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback welcome __@dh7net__!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

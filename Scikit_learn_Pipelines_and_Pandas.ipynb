{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From\n",
    "https://www.kaggle.com/jankoch/scikit-learn-pipelines-and-pandas/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54585743-10b2-ce6f-c396-9134ba654f03",
    "_uuid": "00f2d7fec073205467b6ac92c115e830ad245aa6"
   },
   "source": [
    "Typically, when you want to use the standard pandas/sklearn framework to tackle a machine learning or data analysis problem, you will start analysing the dataset using pandas. Once you've gotten some insights, you may derive a transformed data set using pandas and derive a model using a scikit-learn estimator like Linear or Ridge Regression. Typically, you will estimate the performance of such a model using cross validation.\n",
    "\n",
    "There are several things I don't like with this approach and want to raise awareness of:\n",
    "1. Typically, notebooks contain a lot of repetitive code\n",
    "2. The pandas and scikit-learn frameworks are somehow separated and a combined pipeline is not used\n",
    "3. Most importantly: As it is explained [here]http://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics) the preprocessing step should be tested on a hold out set. This implies, that cross-validation on a dataset requires to perform preprocessing on the respective subsets constructed during cross-validation.\n",
    "    \n",
    "To solve these issues, we only have to find the answer to the following question: Is there a way to perform scikit-learn and pipeline compatible preprocessing using pandas?\n",
    "\n",
    "Luckily, all we need to do is to adhere to the scikit-learn transformers api. A transformer typically contains a transform and a fit method. Using the scikits [TransformerMixin-Class](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html#sklearn.base.TransformerMixin) a fit_transform function is constructed.\n",
    "\n",
    "Let us briefly describe how transformers are used:\n",
    "\n",
    "If the transformer needs to remember the state of the training data, e.g. the mean of a column, the fit method is used on the training data to store this state. Subsequently, the transform function is used on the train and test data. However, if not state preservation is needed, e.g. in the case of log transforming data, the fit functions may essentially do nothing, and we just use the transform function. Note that the fit function is **never** used on the test data\n",
    "\n",
    "So the agenda of this notebooks is as follows:\n",
    "\n",
    "1. Load and read the data\n",
    "2. Define transformer objects (feel free to skip this lengthy paragraph at first)\n",
    "3. Use these transformers to preprocess the data\n",
    "4. Use preprocessing and Ridge regression, gridsearch and crossvalidation to estimate the generalization performance\n",
    "6. Under the assumption, that the gridsearch parameters are stable between cross validation folds, retrain a model using gridsearch on all of the training data\n",
    "7. Further comments on additional transformers, next steps and references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6c7e3f2a-3102-2d81-193f-eea1eca24991",
    "_uuid": "d9ff460abcc556f5f4157f508584be6e3e4d1569"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3b911f21-b14f-722e-77d1-b1d6759b7252",
    "_uuid": "39a8d5f6dcba070d867f4dd75c88886d0b1a79b9",
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File b'../input/train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e5613604df6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File b'../input/train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "y_train = df_train.set_index(\"Id\")[\"SalePrice\"]\n",
    "X_train = df_train.set_index(\"Id\").iloc[:,:-1]\n",
    "X_test = df_test.set_index(\"Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4e751e17-2efd-e175-e931-6f64a739d1dd",
    "_uuid": "cdf0f5451a03b72df2a47f620a4deeb8b8eec299"
   },
   "source": [
    "## Transformer Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "93b5bc31-cb07-470f-9e3a-97251e555171",
    "_uuid": "a606ad9291321ffb66bef36ba4dec8f8a2e449e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "\n",
    "class SelectColumnsTransfomer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" A DataFrame transformer that provides column selection\n",
    "    \n",
    "    Allows to select columns by name from pandas dataframes in scikit-learn\n",
    "    pipelines.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    columns : list of str, names of the dataframe columns to select\n",
    "        Default: [] \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, columns=[]):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\" Selects columns of a DataFrame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        \n",
    "        trans : pandas DataFrame\n",
    "            contains selected columns of X      \n",
    "        \"\"\"\n",
    "        trans = X[self.columns].copy() \n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        \"\"\" Do nothing function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "        y : default None\n",
    "                \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self  \n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "\n",
    "class DataFrameFunctionTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" A DataFrame transformer providing imputation or function application\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    impute : Boolean, default False\n",
    "        \n",
    "    func : function that acts on an array of the form [n_elements, 1]\n",
    "        if impute is True, functions must return a float number, otherwise \n",
    "        an array of the form [n_elements, 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func, impute = False):\n",
    "        self.func = func\n",
    "        self.impute = impute\n",
    "        self.series = pd.Series() \n",
    "\n",
    "    def transform(self, X, **transformparams):\n",
    "        \"\"\" Transforms a DataFrame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        trans : pandas DataFrame\n",
    "            Transformation of X \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.impute:\n",
    "            trans = pd.DataFrame(X).fillna(self.series).copy()\n",
    "        else:\n",
    "            trans = pd.DataFrame(X).apply(self.func).copy()\n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        \"\"\" Fixes the values to impute or does nothing\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "        y : not used, API requirement\n",
    "                \n",
    "        Returns\n",
    "        ----------\n",
    "        self  \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.impute:\n",
    "            self.series = pd.DataFrame(X).apply(self.func).copy()\n",
    "        return self\n",
    "    \n",
    "    \n",
    "class DataFrameFeatureUnion(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" A DataFrame transformer that unites several DataFrame transformers\n",
    "    \n",
    "    Fit several DataFrame transformers and provides a concatenated\n",
    "    Data Frame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_transformers : list of DataFrameTransformers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self, list_of_transformers):\n",
    "        self.list_of_transformers = list_of_transformers\n",
    "        \n",
    "    def transform(self, X, **transformparamn):\n",
    "        \"\"\" Applies the fitted transformers on a DataFrame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        concatted :  pandas DataFrame\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        concatted = pd.concat([transformer.transform(X)\n",
    "                            for transformer in\n",
    "                            self.fitted_transformers_], axis=1).copy()\n",
    "        return concatted\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        \"\"\" Fits several DataFrame Transformers\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "        y : not used, API requirement\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fitted_transformers_ = []\n",
    "        for transformer in self.list_of_transformers:\n",
    "            fitted_trans = clone(transformer).fit(X, y=None, **fitparams)\n",
    "            self.fitted_transformers_.append(fitted_trans)\n",
    "        return self\n",
    "    \n",
    "\n",
    "class ToDummiesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" A Dataframe transformer that provide dummy variable encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    def transform(self, X, **transformparams):\n",
    "        \"\"\" Returns a dummy variable encoded version of a DataFrame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        trans : pandas DataFrame\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        trans = pd.get_dummies(X).copy()\n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        \"\"\" Do nothing operation\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class DropAllZeroTrainColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" A DataFrame transformer that provides dropping all-zero columns\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, **transformparams):\n",
    "        \"\"\" Drops certain all-zero columns of X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        trans : DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        trans = X.drop(self.cols_, axis=1).copy()\n",
    "        return trans\n",
    "\n",
    "    def fit(self, X, y=None, **fitparams):\n",
    "        \"\"\" Determines the all-zero columns of X\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "        y : not used\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cols_ = X.columns[(X==0).all()]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c9a16d6e-c785-1fdf-16b1-fa70bb6f80d2",
    "_uuid": "3cb22305ffde26eccbdc44dab5d372d278d89c56"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0617249a-7d9e-b0c1-5e75-054178f32ae0",
    "_uuid": "1f811934131adb28b03c2990db1435a65ad53fea",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8dab4058-736a-8849-b05d-034e57fcef95",
    "_uuid": "1d3502b68a7addd2d5fbfc1baa949a6a6ec1a807"
   },
   "source": [
    "### Area Columns\n",
    "We start with the columns describing some form of area. As  [Alexandre Paipu](https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models) points out, skewed columns should be log transformed. Checking some data, we find that the skewed columns are essentially the area columns. \n",
    "\n",
    "We simply use a regular expression to filter the area columns, then we transform to float, impute by the mean (even if no missing values appear) and perform a log (x+1) transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d6a2234b-3061-0561-d29c-a33c78cfbf98",
    "_uuid": "b9efc2e2fbe345be776ead51c72553c29c5873a1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "area_cols = X_train.columns[X_train.columns.str.contains('(?i)area|(?i)porch|(?i)sf')].tolist()\n",
    "\n",
    "area_cols_pipeline = make_pipeline(  \n",
    "        SelectColumnsTransfomer(area_cols),\n",
    "        DataFrameFunctionTransformer(func = lambda x: x.astype(np.float64)),\n",
    "        DataFrameFunctionTransformer(func = np.mean, impute=True),\n",
    "        DataFrameFunctionTransformer(func = np.log1p) \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "923a0bd1-bbb5-aa07-4f87-44f5f1c39fd0",
    "_uuid": "d4decabfbd3abd6493cade44d828c0ee894b297d"
   },
   "source": [
    "### Object Columns\n",
    "The object columns are the categorical columns. Reading the data set description, we see that NaN values are allowed for each column. Consequently, we assume that possible levels of the categorical values are known beforehand and that the NaNs are correctly encoded. So though we access the values of the test data, we don't use information not known before the analyzing the data\n",
    "\n",
    "So what we will do here is to determine all possible levels across all categories, construct dummy variables for all variables and levels (this is not efficient!) and then drop the combinations that do not occur. Particularly, when fitting the pipeline to the training data sets and applying to the test sets, we will only keep levels contained in the training data sets!\n",
    "\n",
    "We simply filter the columns by data type, construct the object levels, impute, convert to dummy notation and drop all zero columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5386a390-d40e-f0cf-ff7f-849fb69d5f1d",
    "_uuid": "a7db7a7d2fb52f1269a5ca7e0895214ef283d0f6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object_columns = X_train.columns[X_train.dtypes == object].tolist()\n",
    "object_levels = np.union1d(X_train[object_columns].fillna('NAN'), X_test[object_columns].fillna('NAN'))\n",
    "\n",
    "categorical_cols_pipeline = make_pipeline(\n",
    "        SelectColumnsTransfomer(object_columns),\n",
    "        DataFrameFunctionTransformer(lambda x:'NAN', impute=True),\n",
    "        DataFrameFunctionTransformer(lambda x:x.astype('category', categories=object_levels)),\n",
    "        ToDummiesTransformer(),\n",
    "        DropAllZeroTrainColumnsTransformer()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a1fe39dd-9e02-bcaf-e225-f930abb3a95e",
    "_uuid": "002874250540a35c33fcd7d24442e96d19596b22"
   },
   "source": [
    "### Remaining columns\n",
    "The remaining columns are mostly integer columns. However, if an integers column in a training set has a missing value in the test set the data type will be float in the test set. So, without actually using information beforehand, we just convert the remaining columns to float, store the mean and impute if necessary on train and test sets. \n",
    "\n",
    "Typically, integer columns provide some form of count data, hence we do not use a log transform here. Note however, that the GarageBltYear is also a remaining column. For simplicity, we treat it like the other count-like columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dcc3ccd6-9932-bbde-ee39-4a3da1a16fc2",
    "_uuid": "012573ed1f839760600bd9e5131f7f08bd27528a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remaining_cols = [x for x in X_train.columns.tolist() if x not in object_columns and x not in area_cols]\n",
    "\n",
    "remaining_cols_pipeline = make_pipeline(\n",
    "        SelectColumnsTransfomer(remaining_cols),\n",
    "        DataFrameFunctionTransformer(func = lambda x: x.astype(np.float64)),\n",
    "        DataFrameFunctionTransformer(func = np.mean, impute=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "185904e3-c952-9833-626f-2e0c1886ba4f",
    "_uuid": "e8629246f2b8c404964b9624c45161dbb062df44"
   },
   "source": [
    "### Uniting the pipelines\n",
    "We put the pipelines together using the DataFrameFeatureUnion transformer. To demonstrate that we get a DataFrame we simply use fit_transform on the training set and show the first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "92d793fe-c397-2632-4320-475d7ccfd613",
    "_uuid": "562e8634a7d86580a31df6f96e2b7579d97f28de",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing_features = DataFrameFeatureUnion([area_cols_pipeline, categorical_cols_pipeline, remaining_cols_pipeline])\n",
    "preprocessing_features.fit_transform(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c29fa89f-235a-256c-7fd6-20cd9ffe3ceb",
    "_uuid": "e74e6dba299a701c239946757439d8c52bb1bcf2"
   },
   "source": [
    "## Gridsearch and Crossvalidation\n",
    "We use nested cross validation to estimate the generalization performance. See the 3rd example [here](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "\n",
    "Unfortunately, nested cross validation is not able to return the best model parameters for each fold ([and probably never will be](https://github.com/scikit-learn/scikit-learn/issues/6827)). However, for simplicity, we just assume that the model parameters are stable across the cross validation folds on the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15a3ab03-ea82-7ae0-f40c-a9958fcc1e71",
    "_uuid": "3d37b01e72d4bfcfccf55e77b5abb04e1a59eff1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "pipe_ridge = make_pipeline(preprocessing_features, Ridge())\n",
    "param_grid = {'ridge__alpha' : [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]}\n",
    "pipe_ridge_gs = GridSearchCV(pipe_ridge, param_grid=param_grid, scoring = 'neg_mean_squared_error', cv=3)\n",
    "result = np.sqrt(-cross_val_score(pipe_ridge_gs, X_train, np.log(y_train), scoring = 'neg_mean_squared_error', cv = 5))\n",
    "np.mean(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "514d9d9c-882e-a7bd-6674-9df82d6699d3",
    "_uuid": "13d349ab6ebb6f471e7428b1dbdc408b95962c19"
   },
   "source": [
    "So the result is relatively close to that of [Alexandra Papiu](https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models). But I was not yet able to figure out where the differences come from exactly. One differnece is that I seem to construct more columns.\n",
    "\n",
    "Let me comment on the scoring parameter. Why 'neg_mean_squared_error'? This is a scikit learn convention that ensures that grid search and cross validation always [maximize](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) a specific score.\n",
    "\n",
    "Additionally, you might want to ask, why we don't use a custom scoring function and than use y_train instead of np.log(y_train). \n",
    "The reason is simply, that the loss functions in the algorithms are typically some form of squared loss. Using the np.log transformation, the optimization during model training directly optimizes the correct loss function. If we use a custom scorer object, the internal optimization is performed using standard quadratic error functional and we simply evaluate the results using that custom scorer. Those results do not necessarily have to be the same and perform worse in general (you can even test it here).\n",
    "\n",
    "For completeness, let us just fit an optimized model on the full training data and provide a data set for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68c67d1a-8ed5-1a05-8ed6-4c877f18e615",
    "_uuid": "3c3b0d9978b8f6e55458fdca6a05a20b4d2fb0c7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_ridge_gs.fit(X_train, np.log(y_train))\n",
    "predicted = np.exp(pipe_ridge_gs.predict(X_test))\n",
    "X_test[\"SalePrice\"] = predicted\n",
    "X_test[\"SalePrice\"].reset_index().to_csv('pipe_ridge_gs.csv', index=False)\n",
    "pipe_ridge_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d57e7c86-4ca5-ce9f-3172-d7d429f50e30",
    "_uuid": "84f314790726291a3300900467a866cb04c9d022"
   },
   "source": [
    "## Further comments on additional transformers, next steps and references\n",
    "* The FeatureUnion is not yet able to allow parallel processing\n",
    "* One might want to construct interaction terms only on e.g. the garage columns. If the year is not available, it typically means that no garage is present. So storing the NaNs of such a column and building interaction terms with e.g. the area might provide viable information. To this end, one would need a transformer that selects columns by a somewhat dynamical name pattern after a first FeatureUnion.\n",
    "* I discovered the [sklearn-pandas](https://github.com/paulgb/sklearn-pandas) package just recently. But it seems, that no dataframes but numpy arrays are returned. Thus, the previous idea will be really hard to implement. Nevertheless, it may be easier to just build a transformer\n",
    "* Unit tests are missing. Transformers should e.g. only accept DataFrame objects\n",
    "* I got several ideas from [Zac Stewart's](http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html) blog entry. However, I didn't find source codes for the transformer, so I constructed them on my own ."
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
